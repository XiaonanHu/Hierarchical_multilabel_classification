from gensim.models import KeyedVectors
from tqdm import tqdm
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

def takeSecond(elem):
    return elem[1]

def extract_features(patent_data, extractor, K = None):
    embedding = {}
    if extractor == "glove":
        """
        the features generated by this method is not fixed.
        """
        # load model. It might take a while.
        glove_model = KeyedVectors.load_word2vec_format("../pretrained_model/glove/glove.twitter.27B/glove.twitter.27B.25d.word2vec.txt", binary=False)

        for patent_index in tqdm(patent_data.keys()):
            title = patent_data[patent_index]['title']
            abstract = patent_data[patent_index]['abstract']
            description = patent_data[patent_index]['description']
            # merge all titles
            title = title[0] + ' '+ title[1] + ' ' + title[2]

            # get embeddings
            title_embedding = []
            abstract_embedding = []
            description_embedding = []
            for word in title.split(' '):
                if not word in glove_model.wv.vocab:
                    continue
                title_embedding.append(glove_model[word])
            for word in abstract.split(' '):
                if not word in glove_model.wv.vocab:
                    continue
                abstract_embedding.append(glove_model[word])
            for word in description.split(' '):
                if not word in glove_model.wv.vocab:
                    continue
                description_embedding.append(glove_model[word])

            title_embedding = np.array(title_embedding)
            abstract_embedding = np.array(abstract_embedding)
            description_embedding = np.array(description_embedding)

            embedding[patent_index] = {'title': title_embedding,
                                       'abstract': abstract_embedding,
                                       'description': description_embedding}
            if 'labels' in patent_data[patent_index].keys():
                embedding[patent_index]['labels'] = patent_data[patent_index]['labels']

    elif extractor == "tfidf+glove":
        """
        First use tf-idf to find the top K most informative words in mixture of title, abstract and description.
        Then the glove is utilized to find embeddings for these words.
        """
        # load model. It might take a while.
        glove_model = KeyedVectors.load_word2vec_format(
            "../pretrained_model/glove/glove.twitter.27B/glove.twitter.27B.25d.word2vec.txt", binary=False)

        # Concatenate title, abstract and description into text. Save patent index and its text.
        corpus = []
        patent_index_list = []
        for patent_index in tqdm(patent_data.keys()):
            title = patent_data[patent_index]['title']
            abstract = patent_data[patent_index]['abstract']
            description = patent_data[patent_index]['description']
            # merge all titles
            title = title[0] + ' '+ title[1] + ' ' + title[2]
            text = title + ' ' + abstract + ' ' + description
            corpus.append(text)
            patent_index_list.append(patent_index)

        # tf-idf
        vectorizer = CountVectorizer()
        transformer = TfidfTransformer()
        tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus)).toarray()
        # sort index in descending tfidf order
        informative_index = np.argsort(tfidf, axis=1)[:,::-1]
        # get all the words
        bag_of_words = vectorizer.get_feature_names()

        for i in tqdm(range(len(patent_index_list))):
            embedding_num = 0
            patent_embedding = []
            for informative_word_index in informative_index[i]:
                if not bag_of_words[informative_word_index] in glove_model.wv.vocab:
                    continue
                patent_embedding.append(glove_model[bag_of_words[informative_word_index]])
                embedding_num += 1
                if embedding_num == K:
                    break
            patent_embedding = np.array(patent_embedding)
            embedding[patent_index_list[i]] = patent_embedding

    return embedding

if __name__ == '__main__':
    from get_data import load_data_small
    import os
    labeled_patent_data, unlabeled_patent_data = load_data_small(100)
    print("number of patent:", len(labeled_patent_data))
    features = extract_features(unlabeled_patent_data, extractor = "tfidf+glove", K=20)
    np.save("small_feature.npy", features)
    os.system("pause")